{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Maze\n",
    "source: https://samyzaf.com/ML/rl/qmaze.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](20190526_1740_gif_GOOD_UNTIL_END/seq_movie.gif \"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze, cmap_input='gray'):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap=cmap_input)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qmaze(object):\n",
    "    \"\"\"\n",
    "    Class Description \n",
    "    ------------------\n",
    "    \n",
    "    free_cell == 1.0\n",
    "    wall_cell == 0.0\n",
    "    \n",
    "    _maze: original maze - not altered - only walls + free cells\n",
    "    maze:  maze + way of rat - colored where rat went through, and is now\n",
    "    \n",
    "    functions: \n",
    "    def reset(rat):\n",
    "    def update_state(action):\n",
    "    def valid_actions():\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze_original = np.array(maze)\n",
    "        nrows, ncols = self._maze_original.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        # TODO: position of \"cheese\" can be chosen freely (on a free_cell) // free_cell == 1.0\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze_original[r,c] == 1.0]\n",
    "        # the \"cheese\"=target is not a free_cell - remove\n",
    "        self.free_cells.remove(self.target)\n",
    "        # check: cheese(=target) can't be a wall(=0.0)\n",
    "        if self._maze_original[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "        \n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze_original)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark  # rat_mark: outside variable for coloring rat's way\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size   # row * col\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()  # reset visited cells (it's a set)\n",
    "        \n",
    "    def update_state(self, action):\n",
    "        \"\"\"\n",
    "        Based on Action, udpate rat's position\n",
    "        \n",
    "        also being done:\n",
    "        - mark visited cell\n",
    "        - checks valid movements/actions\n",
    "        \"\"\"\n",
    "        \n",
    "        nrows, ncols = self.maze.shape\n",
    "        # before action - where is the rat:\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:  # if rat is in free cell (before action)\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell (before action)\n",
    "            # print('visited cell/current cell=', self.visited)  # temp\n",
    "\n",
    "        # to where can rat move?\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'   # no where to go\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            nmode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "        \n",
    "        \n",
    "    def valid_actions(self, cell=None):\n",
    "        \"\"\"\n",
    "        Check where rat is\n",
    "        Check where rat can go\n",
    "        \n",
    "        return: possible actions - but rat has NOT YET moved\n",
    "        \"\"\"\n",
    "        if cell is None:  # current cell rat is in\n",
    "            row, col, mode = self.state\n",
    "        else:  # special cell to be checked\n",
    "            row, col = cell\n",
    "            \n",
    "        actions = [0, 1, 2, 3]\n",
    "        # 0: LEFT\n",
    "        # 1: UP\n",
    "        # 2: RIGHT\n",
    "        # 3: DOWN\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:  # top row\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:  # bottom row\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:  # most left col\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:  # most right col\n",
    "            actions.remove(2)\n",
    "        # if cell above is a wall - remove UP:\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        # if cell below is a wall - remove DOWN:\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "        # if cell to the left is a wall - remove LEFT\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        # if cell to the right is a wall - remove RIGHT:\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "        # print('possible actions before acting=', actions)  # temp\n",
    "        return actions\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        returns:\n",
    "            1     - rat found cheese\n",
    "            -33   - mode==blocked ((-0.5 * 8 * 8) - 1 <-- example maze 8*8)\n",
    "                                  (self.min_reward = -0.5 * self.maze.size)\n",
    "            -0.25 - cell already visited\n",
    "            -0.75 - mode==invalid (no change in position)\n",
    "            -0.04 - mode==valid\n",
    "        \"\"\"\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        \n",
    "        # rat found cheese\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0 \n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "        \n",
    "    def game_status(self):\n",
    "        \"\"\"\n",
    "        Example: maze 8*8\n",
    "            min_reward = -0.5 * self.maze.size = -32\n",
    "        Example: maze 3*3\n",
    "            min_reward = .... = -4.5\n",
    "        \"\"\"\n",
    "        # rat cant run indefinitely around the maze:\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # rat found cheese:\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "    \n",
    "    def draw_env(self):\n",
    "        \"\"\"\n",
    "        Clears path of rat\n",
    "        Draws only rat position\n",
    "        \"\"\"\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        # mmor: is this not equal the original _maze ?\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    \n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()  # draws only maze and rat (clears path of rat)\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate  # envstate is only: original maze + rat position\n",
    "\n",
    "    \n",
    "    def act(self, action):\n",
    "        \"\"\"\n",
    "        input: action\n",
    "        output:\n",
    "            envstate: flattened maze + rat (without rat's path)\n",
    "            reward: reward of this particular move\n",
    "            status: new game status (3 possibilities: lose, win, not_over)\n",
    "        \"\"\"\n",
    "        self.update_state(action)  # returns new state: self.state = (nrow of rat, ncol of rat, nmode)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()  # checks total_reward or cheese_found // returns status\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current maze with rat\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0. , 1. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 0. , 1. , 1. , 1. , 0. , 1. , 1. ],\n",
       "       [1. , 1. , 1. , 1. , 0. , 1. , 0. , 1. ],\n",
       "       [1. , 1. , 1. , 0. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 0. , 1. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 1. , 0. , 1. , 0. , 0. , 0. ],\n",
       "       [1. , 1. , 1. , 0. , 1. , 1. , 1. , 1. ],\n",
       "       [1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "\n",
    "print('current maze with rat\\n')\n",
    "qmaze.maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canvas=\n",
      " [[1.  0.  1.  1.  1.  1.  1.  1.  0.5 0.  1.  1.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  0.  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.  1.  1.  1.  0.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  0.  1.  1.  1. ]]\n",
      "reward= -0.04\n",
      "game_over= not_over\n"
     ]
    }
   ],
   "source": [
    "canvas, reward, game_over = qmaze.act(DOWN)  # move down\n",
    "print('canvas=\\n', canvas)\n",
    "print('reward=', reward)\n",
    "print('game_over=', game_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 'valid')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmaze.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n",
      "total_reward=-0.08 out of -32.0\n",
      "state= (2, 0, 'valid')\n",
      "visited= {(1, 0), (0, 0)}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIdB/AQUST1bjYo6aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e658332f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# qmaze = Qmaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "print('total_reward={a} out of {b}'.format(a=qmaze.total_reward, b=qmaze.min_reward))\n",
    "print('state=', qmaze.state)\n",
    "print('visited=', qmaze.visited)\n",
    "show(qmaze, cmap_input='inferno');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABl1JREFUeJzt3U+I3IUZxvF3lkjYZPsnKibFXBoL\nS6HtJauh0BD37kGozdGjp1KKHSgIngpC2iXEm+5NT5JePBR6XEJO1Y0nL0uTmJaFmBpLWxeKqPn1\nYA/twc5ss3mzzy+fz3ngGXb4ZmYgzDsZhqGALAv3+wkAuydcCCRcCCRcCCRcCCRcCCRcCCRcCCRc\nCHRgNw9++OBDw+OHDt6r5/JfPl78et28ebNla3n5RC0t9fwPsp2dySi3uvfGunXjxkd1+/Ynk1mP\n21W4jx86WG8//YP//1ntwts/+nFNp79s2Xrttd/U6TP/bNm6fGlxlFvde2PdOvXky3M9zkdlCCRc\nCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRcCCRc\nCCRcCCRcCCRcCLSrH0Rnb/zsW++07Jx6aaXqTN+/zVeufFCrqz0/Yr+xcbFlZ7+aDMP/Pq0wmUxe\nqKoXqqoee/jIyTd//UrH86q/LR2p7e3tlq3l5SdqaelOy9bOzkL99fonLVuHjx2uRx5rmaqqqlu3\nPhvta9a1NZ1Oa3Pz+t2fIBmGYb2q1quqvn9kafjO797cg6c3W+cJko2Ni63nLH7/ymbL1qmXVurZ\nn/S94144/+FoX7POUy7z8B0XAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkX\nAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAu3bEyTf++bhuvrsUy1bV6/9pW682vND758+\n83y9+MN3W7auHv5uHVj4actW1Zc/Uv75nTdati6c7zt3srZ2rk6fOdayNa99e4Lk0288Ugf//rGt\nu9z645/+3LJV1XsWpPPcyfHjx+vo0YdatuJPkFx95vmydfdbXSdBqnrPgnSeO1lbO1fPnd1f77i+\n40Ig4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg\n4UIg4UIg4UIg4UIg4UIg4UKgXZ0gOXr00ZNvvfV6x/OqnZ2FtnMW3VtbW9datjpPZ1SN+zXr2ron\nJ0hWVk4MXScmLl9abDtn0b011tMZY37Nurbm5aMyBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIu\nBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBJp5yeBBcOXKB7W6\n2ndd4PM7b7RsXb60WFX76xf42RtuB1XVrVuf1fb2dstW5z2fzr9h995Yt9wO2oUL5z8c5T2f7ps3\nY73n43YQsCeEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GEC4GE\nC4GEC4GEC4GEC4GEC4GEC4GEC4GcIKmqkye/3XoW5MDC2ZattbVzdfpMz4+vV/WfchnjVtUXcz3K\nCZL7sLW1da1lq/PcSVX/KZcxbk2n0xqGYeYJkpnh/qeVlRPDH9791V09sXmN9cTE5UuLtbra9477\n8xf73nG7T7mMcavqi7nC9R0XAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkX\nAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAjlBwp7pPuUyxq1TT7481+OcILkPW2M9QTLm\n16xrazqd1ubm9ZmXDGa+4w7DsF5V61VfniAZ61mQzq3O0xnPne07QTLm16xra16+40Ig4UIg4UIg\n4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg\n4UIg4UIg4UIgJ0hGvtV17qSqann5idH+HQ8d+Khlazr9Rb33/j+cIHnQt7rOnVRVbWxcHO3f8alj\nv23ZmpePyhBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBI\nuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBoVydIqmq5qrbu9ZP6t0er6ratmK3uvbFuLQ/D8LVZD5oZ\n7v0ymUw2h2FYsZWx1b33oG/5qAyBhAuB9nO467aitrr3HuitffsdF/hq+/kdF/gKwoVAwoVAwoVA\nwoVA/wLJCVLPX6mNUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e65838cf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze.reset(rat=(0,0))\n",
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze, cmap_input='inferno');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each state consists of all the available cells information, including the rat location. In our Python code, each state is represented by a vector of length 64 (for an 8x8 maze) with gray values 0.0 to 1.0: occupied cells is 0.0, a free cell is 1.0, and the rat cell is 0.5. History (yellow cells) is not recorded since the next move should not depend on past moves! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Class\n",
    "This is the class in which we collect our game episodes (or game experiences) within a memory list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model - a neural network model  \n",
    "- max_memory - maximeal length of episodes to keep. When we reach the maximal lenght of memory, each time we add a new episode, the oldest episode is deleted  \n",
    "- discount factor - this is a special coefficient, usually denoted by γ which is required for the Bellman equation for stochastic environments (in which state transitions are probabilistic). Here is a more practical version of the Bellman equation:  \n",
    "  \n",
    "Q(s,a)=R(s,a)+γ⋅maxi=0,…,n−1Q(s′,ai),(where s′=T(s,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        \"\"\"\n",
    "        model = neural network\n",
    "        max_memory = maximum length of episodes to keep. When we reach the maximal \n",
    "                     length of memory, each time we add a new episode, the oldest \n",
    "                     episode is deleted\n",
    "        discount factor = this is a special coefficient, usually denoted by γ which \n",
    "                     is required for the Bellman equation for stochastic \n",
    "                     environments (in which state transitions are probabilistic).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "        \n",
    "    def remember(self, episode):\n",
    "        \"\"\"\n",
    "        episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        memory[i] = episode\n",
    "        envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        \"\"\"\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "            \n",
    "    def predict(self, envstate):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        \"\"\"\"\"\"\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)  # how many episodes in memory\n",
    "        \n",
    "        # either: how-many-episodes-in-memory   OR    data_size=10\n",
    "        data_size = min(mem_size, data_size)\n",
    "        \n",
    "        inputs = np.zeros((data_size, env_size))  # empty array for model-input\n",
    "        targets = np.zeros((data_size, self.num_actions))  # empty array for model-output\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            # print('get_data(), i=', i, '  --  j=', j)\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                # temp - print('... ...reward=', reward)\n",
    "                # temp - print('... ...discount=', self.discount)\n",
    "                # temp - print('... ...Q_sa=', Q_sa)\n",
    "                # temp - print('... ...Q_sa is max of predict(envstate_next)=', self.predict(envstate_next))\n",
    "                # temp - print('... ...new targets[i, action]=', reward + self.discount * Q_sa)\n",
    "                # temp - print('... ...targets before=', targets[i])\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "                # temp - print('... ...targets[i, action]=', targets[i, action])\n",
    "                # temp - print('... ...targets after=', targets[i])\n",
    "        # temp - print('...memory newly calculated')\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 73, 47, 83,  7,  4, 75, 10, 71, 88, 18, 28, 17, 24, 70, 12, 94,\n",
       "       84, 91,  8, 52, 39])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(100, 22, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory 20190602_1451_gif \n",
      "Successfully created the directory 20190602_1451_gif/pngs \n"
     ]
    }
   ],
   "source": [
    "def create_folder(foldername):\n",
    "    # define the name of the directory to be created\n",
    "    try:  \n",
    "        os.mkdir(foldername)\n",
    "    except OSError:  \n",
    "        print (\"Creation of the directory %s failed\" % foldername)\n",
    "    else:  \n",
    "        print (\"Successfully created the directory %s \" % foldername)\n",
    "\n",
    "\n",
    "now_str = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "folder_name_gif = now_str + '_gif'\n",
    "folder_name_pngs = folder_name_gif + '/pngs'\n",
    "\n",
    "create_folder(folder_name_gif)\n",
    "create_folder(folder_name_pngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e66035a438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SEQUENCE = [DOWN, DOWN, RIGHT, RIGHT, RIGHT, UP]\n",
    "SEQUENCE_LONG = [DOWN, DOWN, RIGHT, RIGHT, RIGHT, \n",
    "                 UP, RIGHT, UP, RIGHT, RIGHT, \n",
    "                 DOWN, RIGHT, DOWN, DOWN, LEFT, \n",
    "                 LEFT, LEFT, DOWN, DOWN, DOWN, \n",
    "                 RIGHT, RIGHT, DOWN, RIGHT]\n",
    "SEQUENCE_INTO_WALL = [DOWN, DOWN, DOWN, DOWN, \n",
    "                      RIGHT, RIGHT, RIGHT, RIGHT, RIGHT]\n",
    "qmaze.reset(rat=(0,0))\n",
    "\n",
    "for seqnum, seq in enumerate(SEQUENCE_INTO_WALL):\n",
    "    qmaze.act(seq)\n",
    "    show(qmaze, cmap_input='inferno');\n",
    "    \n",
    "    filler_str = ''\n",
    "    if seqnum < 10:\n",
    "        filler_str = '0'\n",
    "        \n",
    "    plt.savefig(folder_name_pngs + '/' + filler_str + str(seqnum) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_pngs = os.listdir(folder_name_pngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for filename in filenames_pngs:\n",
    "    images.append(imageio.imread(folder_name_pngs + '/' + filename))\n",
    "imageio.mimsave(folder_name_gif + '/seq_movie.gif', images, duration = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **n_epoch** - Number of training epochs  \n",
    "- **max_memory** - Maximum number of game experiences we keep in memory (see the Experince class above)  \n",
    "- **data_size** - Number of samples we use in each training epoch. This is the number episodes (or game experiences) which we randomly select from our experiences repository (again, see the Experience class above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "def build_model(maze, lr=0.001):\n",
    "    \"\"\"Keras Model\"\"\"\n",
    "    # TODO: make sure, \"maze\" input is correct (array has correct format)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration factor\n",
    "epsilon = 0.1\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)\n",
    "    \n",
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def play_game(model, qmaze, rat_cell):\n",
    "    \"\"\"plays until win or lose\"\"\"\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "    \n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "    \n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "    \n",
    "    print('number of epochs= ', n_epoch)\n",
    "    for epoch in range(n_epoch):\n",
    "        # temp - print('epoch=', epoch, '  --  out of n_epoch=', n_epoch)\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas/maze)\n",
    "        # observe(): makes sure, you only have maze + rat (without path)\n",
    "        #            and then flattens the maze\n",
    "        envstate = qmaze.observe()\n",
    "        # temp - print('starting envstate=', envstate, 'Starting Games\\n')\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            # temp - print('...move/episode=', n_episodes)\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break  # if list of \"valid_actions\" is empty: break\n",
    "            prev_envstate = envstate\n",
    "            \n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                # temp - print('...action chosen at random')\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                # temp - print('...action chosen according to model.predict (argmax)')\n",
    "                # temp - print('...experience.prediction(prev_envstate)=', experience.predict(prev_envstate))\n",
    "                action = np.argmax(experience.predict(prev_envstate))  # model.predict()\n",
    "                \n",
    "            # Apply action, get reward and new envstate\n",
    "            #    output of act:\n",
    "            #      envstate: flattened maze + rat (without rat's path)\n",
    "            #      reward: reward of this particular move\n",
    "            #      status: new game status (3 possibilities: lose, win, not_over)   \n",
    "            # temp - print('...now acting')\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            # temp - print('...valid actions=', valid_actions, ' -- action=', action, '/',actions_dict[action], \n",
    "            # temp -       '  -- new state=', qmaze.state)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "                \n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            # experience.remember():  self.memory.append(episode)\n",
    "            experience.remember(episode)   \n",
    "            n_episodes += 1\n",
    "            \n",
    "            # Train neural network model\n",
    "            # temp - print('... ...checking prediction: ')\n",
    "            # temp - print('...prediction on prev_envstate=', experience.predict(prev_envstate))\n",
    "            # temp - print('...prediction on envstate=     ', experience.predict(envstate))\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            # input= envstate at random\n",
    "            # target= reward + gamma * max_a' Q(s', a')\n",
    "            #      reward= \n",
    "            #               1     - rat found cheese\n",
    "            #               -33   - mode==blocked ((-0.5 * 8 * 8) - 1 <-- example maze 8*8)\n",
    "            #                       (self.min_reward = -0.5 * self.maze.size)\n",
    "            #               -0.25 - cell already visited\n",
    "            #               -0.75 - mode==invalid (no change in position)\n",
    "            #               -0.04 - mode==valid\n",
    "            #      gamma= 0.95\n",
    "            #      max_a' Q(s', a')=  np.max(self.predict(envstate_next))\n",
    "            # temp -- print('...inputs=', inputs)\n",
    "            # temp -- print('...inputs.size=',inputs.size)\n",
    "            # temp - print('...inputs - how many envstates=', inputs.__len__(), '  --  each with ', inputs[0].__len__(), ' elements')\n",
    "            # temp - print('...targets=', targets)\n",
    "            \n",
    "            # temp - print('starting model.fit')\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=2)\n",
    "            \n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "\n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05   # Exploration factor\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "        # temp - print('\\n')\n",
    "        # if n_episodes==19:\n",
    "        #     sys.exit()\n",
    "                \n",
    "                \n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('\\n\\nfiles: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (None, 49)                2450      \n",
      "_________________________________________________________________\n",
      "p_re_lu_41 (PReLU)           (None, 49)                49        \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 49)                2450      \n",
      "_________________________________________________________________\n",
      "p_re_lu_42 (PReLU)           (None, 49)                49        \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 4)                 200       \n",
      "=================================================================\n",
      "Total params: 5,198\n",
      "Trainable params: 5,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_actions = len(actions_dict)\n",
    "\n",
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "qmaze = Qmaze(maze)\n",
    "# show(qmaze)\n",
    "model = build_model(maze)\n",
    "# print(model.summary())\n",
    "\n",
    "# experience = Experience(model, max_memory=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs=  1000\n",
      "Epoch: 000/999 | Loss: 0.0018 | Episodes: 105 | Win count: 0 | Win rate: 0.000 | time: 7.6 seconds\n",
      "Epoch: 001/999 | Loss: 0.0017 | Episodes: 109 | Win count: 0 | Win rate: 0.000 | time: 13.0 seconds\n",
      "Epoch: 002/999 | Loss: 0.0027 | Episodes: 110 | Win count: 0 | Win rate: 0.000 | time: 18.7 seconds\n",
      "Epoch: 003/999 | Loss: 0.0017 | Episodes: 103 | Win count: 0 | Win rate: 0.000 | time: 23.7 seconds\n",
      "Epoch: 004/999 | Loss: 0.0382 | Episodes: 109 | Win count: 0 | Win rate: 0.000 | time: 29.0 seconds\n",
      "Epoch: 005/999 | Loss: 0.0017 | Episodes: 104 | Win count: 0 | Win rate: 0.000 | time: 33.9 seconds\n",
      "Epoch: 006/999 | Loss: 0.0422 | Episodes: 89 | Win count: 1 | Win rate: 0.000 | time: 38.2 seconds\n",
      "Epoch: 007/999 | Loss: 0.0030 | Episodes: 109 | Win count: 1 | Win rate: 0.000 | time: 43.6 seconds\n",
      "Epoch: 008/999 | Loss: 0.0045 | Episodes: 62 | Win count: 2 | Win rate: 0.000 | time: 47.0 seconds\n",
      "Epoch: 009/999 | Loss: 0.0278 | Episodes: 101 | Win count: 2 | Win rate: 0.000 | time: 52.1 seconds\n",
      "Epoch: 010/999 | Loss: 0.0028 | Episodes: 105 | Win count: 2 | Win rate: 0.000 | time: 57.2 seconds\n",
      "Epoch: 011/999 | Loss: 0.0013 | Episodes: 106 | Win count: 2 | Win rate: 0.000 | time: 62.4 seconds\n",
      "Epoch: 012/999 | Loss: 0.0075 | Episodes: 2 | Win count: 3 | Win rate: 0.000 | time: 62.5 seconds\n",
      "Epoch: 013/999 | Loss: 0.0117 | Episodes: 108 | Win count: 3 | Win rate: 0.000 | time: 68.2 seconds\n",
      "Epoch: 014/999 | Loss: 0.0043 | Episodes: 106 | Win count: 3 | Win rate: 0.000 | time: 73.4 seconds\n",
      "Epoch: 015/999 | Loss: 0.0017 | Episodes: 8 | Win count: 4 | Win rate: 0.000 | time: 73.8 seconds\n",
      "Epoch: 016/999 | Loss: 0.0137 | Episodes: 12 | Win count: 5 | Win rate: 0.000 | time: 74.4 seconds\n",
      "Epoch: 017/999 | Loss: 0.0028 | Episodes: 2 | Win count: 6 | Win rate: 0.000 | time: 74.5 seconds\n",
      "Epoch: 018/999 | Loss: 0.0254 | Episodes: 108 | Win count: 6 | Win rate: 0.000 | time: 79.9 seconds\n",
      "Epoch: 019/999 | Loss: 0.0347 | Episodes: 108 | Win count: 6 | Win rate: 0.000 | time: 85.6 seconds\n",
      "Epoch: 020/999 | Loss: 0.0534 | Episodes: 3 | Win count: 7 | Win rate: 0.000 | time: 85.7 seconds\n",
      "Epoch: 021/999 | Loss: 0.0216 | Episodes: 105 | Win count: 7 | Win rate: 0.000 | time: 90.9 seconds\n",
      "Epoch: 022/999 | Loss: 0.0009 | Episodes: 104 | Win count: 7 | Win rate: 0.000 | time: 96.1 seconds\n",
      "Epoch: 023/999 | Loss: 0.0836 | Episodes: 3 | Win count: 8 | Win rate: 0.000 | time: 96.2 seconds\n",
      "Epoch: 024/999 | Loss: 0.0035 | Episodes: 2 | Win count: 9 | Win rate: 0.375 | time: 96.3 seconds\n",
      "Epoch: 025/999 | Loss: 0.0022 | Episodes: 103 | Win count: 9 | Win rate: 0.375 | time: 101.4 seconds\n",
      "Epoch: 026/999 | Loss: 0.0014 | Episodes: 102 | Win count: 9 | Win rate: 0.375 | time: 106.5 seconds\n",
      "Epoch: 027/999 | Loss: 0.0010 | Episodes: 102 | Win count: 9 | Win rate: 0.375 | time: 111.9 seconds\n",
      "Epoch: 028/999 | Loss: 0.0012 | Episodes: 101 | Win count: 9 | Win rate: 0.375 | time: 116.9 seconds\n",
      "Epoch: 029/999 | Loss: 0.0021 | Episodes: 104 | Win count: 9 | Win rate: 0.375 | time: 122.0 seconds\n",
      "Epoch: 030/999 | Loss: 0.0023 | Episodes: 104 | Win count: 9 | Win rate: 0.333 | time: 127.3 seconds\n",
      "Epoch: 031/999 | Loss: 0.0034 | Episodes: 109 | Win count: 9 | Win rate: 0.333 | time: 132.6 seconds\n",
      "Epoch: 032/999 | Loss: 0.0018 | Episodes: 107 | Win count: 9 | Win rate: 0.292 | time: 137.9 seconds\n",
      "Epoch: 033/999 | Loss: 0.0258 | Episodes: 41 | Win count: 10 | Win rate: 0.333 | time: 139.9 seconds\n",
      "Epoch: 034/999 | Loss: 0.0197 | Episodes: 105 | Win count: 10 | Win rate: 0.333 | time: 145.0 seconds\n",
      "Epoch: 035/999 | Loss: 0.0024 | Episodes: 2 | Win count: 11 | Win rate: 0.375 | time: 145.1 seconds\n",
      "Epoch: 036/999 | Loss: 0.0044 | Episodes: 104 | Win count: 11 | Win rate: 0.333 | time: 150.3 seconds\n",
      "Epoch: 037/999 | Loss: 0.0022 | Episodes: 101 | Win count: 11 | Win rate: 0.333 | time: 155.3 seconds\n",
      "Epoch: 038/999 | Loss: 0.0031 | Episodes: 5 | Win count: 12 | Win rate: 0.375 | time: 155.5 seconds\n",
      "Epoch: 039/999 | Loss: 0.0054 | Episodes: 61 | Win count: 13 | Win rate: 0.375 | time: 158.5 seconds\n",
      "Epoch: 040/999 | Loss: 0.0198 | Episodes: 105 | Win count: 13 | Win rate: 0.333 | time: 163.7 seconds\n",
      "Epoch: 041/999 | Loss: 0.0223 | Episodes: 104 | Win count: 13 | Win rate: 0.292 | time: 168.9 seconds\n",
      "Epoch: 042/999 | Loss: 0.0062 | Episodes: 4 | Win count: 14 | Win rate: 0.333 | time: 169.1 seconds\n",
      "Epoch: 043/999 | Loss: 0.0013 | Episodes: 3 | Win count: 15 | Win rate: 0.375 | time: 169.2 seconds\n",
      "Epoch: 044/999 | Loss: 0.0177 | Episodes: 102 | Win count: 15 | Win rate: 0.333 | time: 174.3 seconds\n",
      "Epoch: 045/999 | Loss: 0.0013 | Episodes: 104 | Win count: 15 | Win rate: 0.333 | time: 179.5 seconds\n",
      "Epoch: 046/999 | Loss: 0.0047 | Episodes: 3 | Win count: 16 | Win rate: 0.375 | time: 179.6 seconds\n",
      "Epoch: 047/999 | Loss: 0.0224 | Episodes: 104 | Win count: 16 | Win rate: 0.333 | time: 184.7 seconds\n",
      "Epoch: 048/999 | Loss: 0.0016 | Episodes: 6 | Win count: 17 | Win rate: 0.333 | time: 185.0 seconds\n",
      "Epoch: 049/999 | Loss: 0.0015 | Episodes: 103 | Win count: 17 | Win rate: 0.333 | time: 190.1 seconds\n",
      "Epoch: 050/999 | Loss: 0.0016 | Episodes: 104 | Win count: 17 | Win rate: 0.333 | time: 195.2 seconds\n",
      "Epoch: 051/999 | Loss: 0.0017 | Episodes: 104 | Win count: 17 | Win rate: 0.333 | time: 200.3 seconds\n",
      "Epoch: 052/999 | Loss: 0.0012 | Episodes: 109 | Win count: 17 | Win rate: 0.333 | time: 205.5 seconds\n",
      "Epoch: 053/999 | Loss: 0.0015 | Episodes: 104 | Win count: 17 | Win rate: 0.333 | time: 210.6 seconds\n",
      "Epoch: 054/999 | Loss: 0.0513 | Episodes: 104 | Win count: 17 | Win rate: 0.333 | time: 215.6 seconds\n",
      "Epoch: 055/999 | Loss: 0.0655 | Episodes: 104 | Win count: 17 | Win rate: 0.333 | time: 220.6 seconds\n",
      "Epoch: 056/999 | Loss: 0.0014 | Episodes: 104 | Win count: 17 | Win rate: 0.333 | time: 225.7 seconds\n",
      "Epoch: 057/999 | Loss: 0.0015 | Episodes: 17 | Win count: 18 | Win rate: 0.333 | time: 226.5 seconds\n",
      "Epoch: 058/999 | Loss: 0.0054 | Episodes: 109 | Win count: 18 | Win rate: 0.333 | time: 231.8 seconds\n",
      "Epoch: 059/999 | Loss: 0.0377 | Episodes: 108 | Win count: 18 | Win rate: 0.292 | time: 236.9 seconds\n",
      "Epoch: 060/999 | Loss: 0.0032 | Episodes: 1 | Win count: 19 | Win rate: 0.333 | time: 237.0 seconds\n",
      "Epoch: 061/999 | Loss: 0.0378 | Episodes: 69 | Win count: 20 | Win rate: 0.375 | time: 240.3 seconds\n",
      "Epoch: 062/999 | Loss: 0.0046 | Episodes: 2 | Win count: 21 | Win rate: 0.375 | time: 240.4 seconds\n",
      "Epoch: 063/999 | Loss: 0.0109 | Episodes: 26 | Win count: 22 | Win rate: 0.375 | time: 241.6 seconds\n",
      "Epoch: 064/999 | Loss: 0.0044 | Episodes: 102 | Win count: 22 | Win rate: 0.375 | time: 246.5 seconds\n",
      "Epoch: 065/999 | Loss: 0.0029 | Episodes: 1 | Win count: 23 | Win rate: 0.417 | time: 246.5 seconds\n",
      "Epoch: 066/999 | Loss: 0.0211 | Episodes: 2 | Win count: 24 | Win rate: 0.417 | time: 246.6 seconds\n",
      "Epoch: 067/999 | Loss: 0.0037 | Episodes: 2 | Win count: 25 | Win rate: 0.417 | time: 246.7 seconds\n",
      "Epoch: 068/999 | Loss: 0.0126 | Episodes: 103 | Win count: 25 | Win rate: 0.417 | time: 251.6 seconds\n",
      "Epoch: 069/999 | Loss: 0.0055 | Episodes: 34 | Win count: 26 | Win rate: 0.458 | time: 253.3 seconds\n",
      "Epoch: 070/999 | Loss: 0.0264 | Episodes: 6 | Win count: 27 | Win rate: 0.458 | time: 253.6 seconds\n",
      "Epoch: 071/999 | Loss: 0.0059 | Episodes: 1 | Win count: 28 | Win rate: 0.500 | time: 253.6 seconds\n",
      "Epoch: 072/999 | Loss: 0.0018 | Episodes: 18 | Win count: 29 | Win rate: 0.500 | time: 254.5 seconds\n",
      "Epoch: 073/999 | Loss: 0.0035 | Episodes: 71 | Win count: 30 | Win rate: 0.542 | time: 257.9 seconds\n",
      "Epoch: 074/999 | Loss: 0.0030 | Episodes: 25 | Win count: 31 | Win rate: 0.583 | time: 259.1 seconds\n",
      "Epoch: 075/999 | Loss: 0.0032 | Episodes: 3 | Win count: 32 | Win rate: 0.625 | time: 259.3 seconds\n",
      "Epoch: 076/999 | Loss: 0.0009 | Episodes: 14 | Win count: 33 | Win rate: 0.667 | time: 260.0 seconds\n",
      "Epoch: 077/999 | Loss: 0.0021 | Episodes: 28 | Win count: 34 | Win rate: 0.708 | time: 261.3 seconds\n",
      "Epoch: 078/999 | Loss: 0.0020 | Episodes: 19 | Win count: 35 | Win rate: 0.750 | time: 262.3 seconds\n",
      "Epoch: 079/999 | Loss: 0.0011 | Episodes: 24 | Win count: 36 | Win rate: 0.792 | time: 263.4 seconds\n",
      "Epoch: 080/999 | Loss: 0.0014 | Episodes: 20 | Win count: 37 | Win rate: 0.833 | time: 264.4 seconds\n",
      "Epoch: 081/999 | Loss: 0.0017 | Episodes: 24 | Win count: 38 | Win rate: 0.833 | time: 265.5 seconds\n",
      "Epoch: 082/999 | Loss: 0.0015 | Episodes: 55 | Win count: 39 | Win rate: 0.875 | time: 268.2 seconds\n",
      "Epoch: 083/999 | Loss: 0.0018 | Episodes: 12 | Win count: 40 | Win rate: 0.917 | time: 268.8 seconds\n",
      "Epoch: 084/999 | Loss: 0.0012 | Episodes: 2 | Win count: 41 | Win rate: 0.917 | time: 268.9 seconds\n",
      "Epoch: 085/999 | Loss: 0.0010 | Episodes: 6 | Win count: 42 | Win rate: 0.917 | time: 269.1 seconds\n",
      "Epoch: 086/999 | Loss: 0.0037 | Episodes: 10 | Win count: 43 | Win rate: 0.917 | time: 269.6 seconds\n",
      "Epoch: 087/999 | Loss: 0.0018 | Episodes: 6 | Win count: 44 | Win rate: 0.917 | time: 269.9 seconds\n",
      "Epoch: 088/999 | Loss: 0.0019 | Episodes: 14 | Win count: 45 | Win rate: 0.958 | time: 270.6 seconds\n",
      "Epoch: 089/999 | Loss: 0.0012 | Episodes: 28 | Win count: 46 | Win rate: 0.958 | time: 272.0 seconds\n",
      "Epoch: 090/999 | Loss: 0.0020 | Episodes: 3 | Win count: 47 | Win rate: 0.958 | time: 272.1 seconds\n",
      "Epoch: 091/999 | Loss: 0.0008 | Episodes: 3 | Win count: 48 | Win rate: 0.958 | time: 272.3 seconds\n",
      "Epoch: 092/999 | Loss: 0.0040 | Episodes: 3 | Win count: 49 | Win rate: 1.000 | time: 272.4 seconds\n",
      "Epoch: 093/999 | Loss: 0.0015 | Episodes: 2 | Win count: 50 | Win rate: 1.000 | time: 272.6 seconds\n",
      "Epoch: 094/999 | Loss: 0.0009 | Episodes: 2 | Win count: 51 | Win rate: 1.000 | time: 272.7 seconds\n",
      "Epoch: 095/999 | Loss: 0.0013 | Episodes: 12 | Win count: 52 | Win rate: 1.000 | time: 273.4 seconds\n",
      "Epoch: 096/999 | Loss: 0.0008 | Episodes: 26 | Win count: 53 | Win rate: 1.000 | time: 274.7 seconds\n",
      "Epoch: 097/999 | Loss: 0.0006 | Episodes: 15 | Win count: 54 | Win rate: 1.000 | time: 275.6 seconds\n",
      "Epoch: 098/999 | Loss: 0.0021 | Episodes: 1 | Win count: 55 | Win rate: 1.000 | time: 275.7 seconds\n",
      "Epoch: 099/999 | Loss: 0.0008 | Episodes: 34 | Win count: 56 | Win rate: 1.000 | time: 277.4 seconds\n",
      "Epoch: 100/999 | Loss: 0.0025 | Episodes: 3 | Win count: 57 | Win rate: 1.000 | time: 277.6 seconds\n",
      "Epoch: 101/999 | Loss: 0.0006 | Episodes: 3 | Win count: 58 | Win rate: 1.000 | time: 277.8 seconds\n",
      "Reached 100% win rate at epoch: 101\n",
      "\n",
      "\n",
      "files: model.h5, model.json\n",
      "n_epoch: 101, max_mem: 392, data: 32, time: 278.5 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "278.477652"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default settings:\n",
    "# n_epoch=15'000 | max_memory=1000 | data_size=50\n",
    "qtrain(model=model, maze=maze, n_epoch=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmaze_result = Qmaze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABU5JREFUeJzt3b+KU2kcx+H3LKIQXWwWTjOlEPuk\nFeJVeAdewWm9g1gLXsH0XsDkAiaF5XQWggxMOdbvFgq7IG4m/pnffuc8D6QLfI8mH52pfkPvvQFZ\n/qh+AOB4woVAwoVAwoVAwoVAwoVAwoVAwoVAwoVA94558/379/tisfhdz/KfFotF+/TpU8n206dP\n28OHD0u2P3/+bHtG2x8+fGhXV1fDofcdFe5isWjPnj378af6CZvNpk3TVLL95s2bttlsSrZ3u53t\nGW2v1+sbvc+PyhBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBI\nuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBDoqKNfT548ae/evftdz/Kfdrtd672X\nbVfZ7/ft+fPnJdvb7bZ0u+rwVmutDcPBg3mlhkMxDMPwsrX2srXWxnFcnZ6e3sZzfeP6+ro9evRo\ndtuXl5ft48ePJdsnJyel2+M4lmxfX1+3i4uLku1pmlrv/fC/Gr33G79Wq1WvcnZ2Nsvt7XbbW2sl\nr+rtKmdnZ2V/7i9JHm7R77gQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQ\nSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQ6Khw9/t9G4ah5DXX7dVq\nddRhtl/5qt7m+446s/n48ePVq1evbuO5vlF98rFqe7lczvK8aPX2nTqz2QpPD1affKzanut50ert\nyu96d2YT7ibhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDh\nQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQqCjwq0+uzjH7WpzPG263+9Lv2s3+lwOfTn+\nfWZzHMfV6enpT38ZfkT12cW5bledm6w+qzqOY8n2NE3t/Pz8157ZXK1WvUr12cW5brcZnjbdbrdl\nf+dfGzvYot9xIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBw\nIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIVDMmc3Ly8vSs4tz3a46N1l9XrRq\n+86d2aw+uzjX7SrV50WrOLMJd5hwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBw\nIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZAzmzewXC5nefLR\n9u1zZvMXvuZ68tH27XNmE+4w4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg\n4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UKgmDObcz27WHle9OTk\npI3jWLJd/Xk/ePCgZHuapvb+/fuDZzbvHXpD7/1ta+1ta62t1+u+2Wx+/ul+wG63a3Pcfv36dZum\nqWR7u922Fy9elGxXf97L5bJk+6b8qAyBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuB\nhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBjjqz2Vpb\nttYufvdDfcdfrbUr27bv+Pay9/7noTcdDPf/YhiG89772rZt235UhkjChUBJ4b61bdv2FzG/4wL/\nSPofF/hKuBBIuBBIuBBIuBDob/Kuv6l+4Z2QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e670d7f128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(qmaze_result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02604586, -0.17031717,  0.00070082, -0.3416536 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(maze.reshape(1,-1))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_argmax = np.argmax(pred)  # Returns the indices of the maximum values along an axis.\n",
    "pred_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_dict[pred_argmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.5, 0. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0. , 0. , 1. ,\n",
       "         0. , 0. , 0. , 0. , 1. , 1. , 1. , 0. , 1. , 1. , 1. , 1. , 0. ,\n",
       "         0. , 1. , 1. , 0. , 0. , 0. , 1. , 1. , 1. , 1. , 0. , 1. , 1. ,\n",
       "         1. , 1. , 1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. ]]),\n",
       " -0.25,\n",
       " 'not_over')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmaze_result.act(pred_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABU5JREFUeJzt3b+KU2kcx+H3LKIQXWwWTjOlEPuk\nFeJVeAdewWm9g1gLXsH0XsDkAiaF5XQWggxMOdbvFgq7IG4m/pnffuc8D6QLfI8mH52pfkPvvQFZ\n/qh+AOB4woVAwoVAwoVAwoVAwoVAwoVAwoVAwoVA94558/379/tisfhdz/KfFotF+/TpU8n206dP\n28OHD0u2P3/+bHtG2x8+fGhXV1fDofcdFe5isWjPnj378af6CZvNpk3TVLL95s2bttlsSrZ3u53t\nGW2v1+sbvc+PyhBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBI\nuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBDoqKNfT548ae/evftdz/Kfdrtd672X\nbVfZ7/ft+fPnJdvb7bZ0u+rwVmutDcPBg3mlhkMxDMPwsrX2srXWxnFcnZ6e3sZzfeP6+ro9evRo\ndtuXl5ft48ePJdsnJyel2+M4lmxfX1+3i4uLku1pmlrv/fC/Gr33G79Wq1WvcnZ2Nsvt7XbbW2sl\nr+rtKmdnZ2V/7i9JHm7R77gQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQ\nSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQSLgQ6Khw9/t9G4ah5DXX7dVq\nddRhtl/5qt7m+446s/n48ePVq1evbuO5vlF98rFqe7lczvK8aPX2nTqz2QpPD1affKzanut50ert\nyu96d2YT7ibhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDh\nQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQiDhQqCjwq0+uzjH7WpzPG263+9Lv2s3+lwOfTn+\nfWZzHMfV6enpT38ZfkT12cW5bledm6w+qzqOY8n2NE3t/Pz8157ZXK1WvUr12cW5brcZnjbdbrdl\nf+dfGzvYot9xIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBw\nIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIVDMmc3Ly8vSs4tz3a46N1l9XrRq\n+86d2aw+uzjX7SrV50WrOLMJd5hwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBw\nIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZBwIZAzmzewXC5nefLR\n9u1zZvMXvuZ68tH27XNmE+4w4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg\n4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UKgmDObcz27WHle9OTk\npI3jWLJd/Xk/ePCgZHuapvb+/fuDZzbvHXpD7/1ta+1ta62t1+u+2Wx+/ul+wG63a3Pcfv36dZum\nqWR7u922Fy9elGxXf97L5bJk+6b8qAyBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuB\nhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBhAuBjjqz2Vpb\nttYufvdDfcdfrbUr27bv+Pay9/7noTcdDPf/YhiG89772rZt235UhkjChUBJ4b61bdv2FzG/4wL/\nSPofF/hKuBBIuBBIuBBIuBDob/Kuv6l+4Z2QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e66f8cf080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(qmaze_result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game(model=model, qmaze=Qmaze(maze), rat_cell=(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
